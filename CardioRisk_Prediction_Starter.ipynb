{"cells":[{"cell_type":"markdown","id":"t4kw2y4G7fTt","metadata":{"id":"t4kw2y4G7fTt"},"source":["# CardioRisk Prediction"]},{"cell_type":"markdown","id":"B_pvJBye9QBq","metadata":{"id":"B_pvJBye9QBq"},"source":["## Problem Statement\n","\n","CardioCare, a healthcare provider, is committed to enhancing preventative care and improving patient outcomes. With the growing prevalence of cardiovascular disease (CVD), accurate and timely risk assessment is essential. Although CardioCare might provide comprehensive medical resources, optimising doctors' valuable consultation time is crucial for efficient and effective care. At the same time, correctly identifying at-risk patients is highly important."]},{"cell_type":"markdown","id":"gZ2ZOOUC94R0","metadata":{"id":"gZ2ZOOUC94R0"},"source":["### Business Objective\n","\n","CardioCare aims to develop a machine learning model to predict CVD risk using patient health data. This model is intended to support healthcare providers in efficiently allocating resources and optimising doctors' consultation time. By identifying patients with high risk of CVD, the model can help prioritise consultations and potentially eliminate the need for an initial consultation stage for some patients. This will allow doctors to focus their expertise on individuals requiring immediate attention."]},{"cell_type":"markdown","id":"mWJl0uKj-D4X","metadata":{"id":"mWJl0uKj-D4X"},"source":["### Assignment Tasks\n","\n","You need to perform the following steps to complete this assignment:\n","1. Data Understanding\n","2. Data Cleaning\n","3. Exploratory Data Analysis\n","4. Train Validation Split\n","5. Feature Engineering\n","6. Model Building\n","8. Prediction and Model Evaluation"]},{"cell_type":"markdown","id":"qFX4JrIoPaqr","metadata":{"id":"qFX4JrIoPaqr"},"source":["**Based on this assignment, you have to answer the following questions:**\n","\n","- What insights can we gain from exploring the relationships between different health metrics and the prevalence of cardiovascular disease within the patient population?\n","\n","- Based on the analysis, which patient characteristics emerge as the strongest predictors of cardiovascular disease risk? Are there any surprising or unexpected findings?\n","\n","- How effectively can machine learning models identify individuals at risk of developing cardiovascular disease based on their health data? How does the evaluation results vary across different models?\n","\n","- How can CardioCare integrate the predictive model into their existing healthcare workflows to enhance preventative care strategies?"]},{"cell_type":"markdown","id":"yPGEzn5DTfj8","metadata":{"id":"yPGEzn5DTfj8"},"source":["### Data Dictionary\n","\n","The CardioRisk Prediction has 14 Columns and 70000 Rows. Following data dictionary provides the description for each column present in dataset:\n","\n","\n","<table>\n","  <tr>\n","    <th>Column Name</th>\n","    <th>Description</th>\n","  </tr>\n","  <tr>\n","    <td>Unnamed: 0</td>\n","    <td>Index or row number</td>\n","  </tr>\n","  <tr>\n","    <td>id</td>\n","    <td>Unique identifier for each individual in the dataset</td>\n","  </tr>\n","  <tr>\n","    <td>age</td>\n","    <td>Age of the individual, measured in days</td>\n","  </tr>\n","  <tr>\n","    <td>gender</td>\n","    <td>Gender of the individual (1: Female, 2: Male)</td>\n","  </tr>\n","  <tr>\n","    <td>height</td>\n","    <td>Height of the individual, measured in centimeters</td>\n","  </tr>\n","  <tr>\n","    <td>weight</td>\n","    <td>Weight of the individual, measured in kilograms</td>\n","  </tr>\n","  <tr>\n","    <td>ap_hi</td>\n","    <td>Systolic blood pressure reading</td>\n","  </tr>\n","  <tr>\n","    <td>ap_lo</td>\n","    <td>Diastolic blood pressure reading</td>\n","  </tr>\n","  <tr>\n","    <td>cholesterol</td>\n","    <td>Cholesterol level (0: normal, 1: above normal, 2: well above normal)</td>\n","  </tr>\n","  <tr>\n","    <td>gluc</td>\n","    <td>Glucose level (0: normal, 1: above normal, 2: well above normal)</td>\n","  </tr>\n","  <tr>\n","    <td>smoke</td>\n","    <td>Smoking status (0: No, 1: Yes)</td>\n","  </tr>\n","  <tr>\n","    <td>alco</td>\n","    <td>Alcohol intake status (0: No, 1: Yes)</td>\n","  </tr>\n","  <tr>\n","    <td>active</td>\n","    <td>Physical activity status (0: No, 1: Yes)</td>\n","  </tr>\n","  <tr>\n","    <td>cardio</td>\n","    <td>Presence or absence of cardiovascular disease (0: No Disease, 1: Disease)</td>\n","  </tr>\n","</table>\n","\n","</body>\n","</html>\n","\n","\n","    \n","This data dictionary serves as a reference for understanding the dataset and its variables."]},{"cell_type":"markdown","source":["# Parameters\n","\n","- Learned: Estimated automatically during training from the data.\n","- Internal: Part of the model itself.\n","- Examples: Weights and biases in a neural network, coefficients in regression.\n","- Role: Used by the model to make predictions.\n","\n","# Hyperparameters\n","- Set Externally: Chosen by the data scientist before training starts.\n","- Control Learning: Influence the training algorithm's behavior.\n","- Examples: Learning rate, number of epochs, batch size, K in KNN, tree depth in Random Forests.\n","- Role: Tune the model's performance, prevent overfitting/underfitting.\n","\n","# Key Difference in Action\n","- Training: The algorithm adjusts parameters (weights) to fit data.\n","- Tuning: You try different hyperparameters (e.g., learning rate) to find the best settings for the training process, then retrain to find optimal parameters for those settings."],"metadata":{"id":"-oONXzNftlI1"},"id":"-oONXzNftlI1"},{"cell_type":"markdown","id":"H85rQZ1n-n6i","metadata":{"id":"H85rQZ1n-n6i"},"source":["## **1. Data Understanding**\n","\n","<font color = red>[2 marks]</font> <br>\n","\n","In this stage, you have to load the dataset and check basic statistics of the data, including preview of data, dimension of data, column descriptions and data types."]},{"cell_type":"code","execution_count":null,"id":"36a142d8","metadata":{"id":"36a142d8"},"outputs":[],"source":["# suggested imports; import more libraries as needed\n","import pandas as pd, numpy as np\n","import matplotlib.pyplot as plt, seaborn as sns\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, \\\n","    confusion_matrix, classification_report, roc_curve, roc_auc_score\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","id":"2F6zFVytJLU9","metadata":{"id":"2F6zFVytJLU9"},"source":["### **1.1 Load the dataset**\n","\n","<font color = red>[2 marks]</font> <br>"]},{"cell_type":"code","execution_count":null,"id":"lKoOgY-Yzc1q","metadata":{"collapsed":true,"id":"lKoOgY-Yzc1q"},"outputs":[],"source":["# Load the dataset\n"]},{"cell_type":"markdown","id":"CR097E8dJTnK","metadata":{"id":"CR097E8dJTnK"},"source":["#### **1.1.1** Check the first few entries"]},{"cell_type":"code","execution_count":null,"id":"A_y4R1f-BWQV","metadata":{"collapsed":true,"id":"A_y4R1f-BWQV"},"outputs":[],"source":["# Check the first few entries\n"]},{"cell_type":"markdown","id":"lyxZxdeCYXI-","metadata":{"id":"lyxZxdeCYXI-"},"source":["#### **1.1.2** Remove columns which are irrelevant <font color = red>[2 marks]</font> <br>"]},{"cell_type":"code","execution_count":null,"id":"LEO4ZEhHYXTu","metadata":{"id":"LEO4ZEhHYXTu"},"outputs":[],"source":["# Remove irrelevant columns like unique identifiers or index\n"]},{"cell_type":"markdown","id":"5ILcGyj7L86c","metadata":{"id":"5ILcGyj7L86c"},"source":["#### **1.1.3** Inspect the shape of the dataset"]},{"cell_type":"code","execution_count":null,"id":"rxziy_KyBWVc","metadata":{"collapsed":true,"id":"rxziy_KyBWVc"},"outputs":[],"source":["# Inspect the shape of the dataset\n"]},{"cell_type":"markdown","id":"qZwfHqSZMCn8","metadata":{"id":"qZwfHqSZMCn8"},"source":["#### **1.1.4** Inspect the different columns in the dataset"]},{"cell_type":"code","execution_count":null,"id":"l9VLYc2PBWcJ","metadata":{"collapsed":true,"id":"l9VLYc2PBWcJ"},"outputs":[],"source":["# Inspect the different columns in the dataset\n"]},{"cell_type":"markdown","id":"-rlA-sxmBi6F","metadata":{"id":"-rlA-sxmBi6F"},"source":["Check the summary of the dataset"]},{"cell_type":"code","execution_count":null,"id":"kXQkPZUlBl3L","metadata":{"collapsed":true,"id":"kXQkPZUlBl3L"},"outputs":[],"source":["# Check the summary of the dataset\n"]},{"cell_type":"markdown","id":"MxaQRWEg0Fxb","metadata":{"id":"MxaQRWEg0Fxb"},"source":["## **2. Data Cleaning**\n","\n","<font color = red>[8 marks]</font> <br>"]},{"cell_type":"markdown","id":"VBVcKNwYOsJ8","metadata":{"id":"VBVcKNwYOsJ8"},"source":["### **2.1 Identify and handle redundant or invalid/illogical physiological values**\n","\n","<font color = red>[6 marks]</font> <br>\n","\n","Examine the dataset to identify any columns containing data points that are invalid, illogical, or fall outside of typical physiological ranges.\n","\n","- Pay attention to blood pressure values and ensure they fall within reasonable physiological limits. Very high or low values might need to be investigated or addressed. Blood pressure values less than 30 and more than 300 are rarely observed.\n","- Additionally, think about which unit might be more intuitive for understanding a person's age in a healthcare context.\n","- Similarly, reflect on the representation of height and explore whether using a different unit would align better with typical practices in healthcare and enhance the overall interpretability of the data."]},{"cell_type":"markdown","id":"ZOoTjqiavzxd","metadata":{"id":"ZOoTjqiavzxd"},"source":["#### **2.1.1** Check the statistical summary of the data <font color = red>[1 marks]</font> <br>\n","\n","Examine the statistical summary to identify the columns containing data points that are invalid, illogical, or fall outside of typical physiological ranges."]},{"cell_type":"code","execution_count":null,"id":"N6127C8os1L4","metadata":{"collapsed":true,"id":"N6127C8os1L4"},"outputs":[],"source":["# Check the statistical summary of the data\n"]},{"cell_type":"markdown","id":"zeHvx3zYqxha","metadata":{"id":"zeHvx3zYqxha"},"source":["#### **2.1.2** Handle rows with invalid/illogical values <font color = red>[3 marks]</font> <br>\n","\n","Based on the details of data present in statistical summary, handle the columns that have invalid/illogical values or does not fall within physiological limits or have extreme values."]},{"cell_type":"code","execution_count":null,"id":"1mKrE__umrvp","metadata":{"id":"1mKrE__umrvp"},"outputs":[],"source":["# Handle rows which have invalid or illogical values or does not fall within physiological limits (include extreme cases) for blood pressure and height etc\n","\n"]},{"cell_type":"markdown","id":"wFlAAL01soEv","metadata":{"id":"wFlAAL01soEv"},"source":["#### **2.1.3** Modify the representation of patient age and height <font color = red>[2 marks]</font> <br>"]},{"cell_type":"code","execution_count":null,"id":"SAZZV3RsOgMM","metadata":{"id":"SAZZV3RsOgMM"},"outputs":[],"source":["# Modify the representation of patient age and height (to years and meters) for better understanding in a healthcare context\n","\n"]},{"cell_type":"markdown","id":"BMck5-4TrhjO","metadata":{"id":"BMck5-4TrhjO"},"source":["### **2.2 Fix DataTypes**\n","\n","<font color = red>[2 marks]</font> <br>"]},{"cell_type":"markdown","id":"23fa8367","metadata":{"id":"23fa8367"},"source":["#### **2.2.1** Review and fix the data types of all columns <font color = red>[2 marks]</font> <br>\n","\n","Ensuring the columns accurately reflect the nature of the data"]},{"cell_type":"code","execution_count":null,"id":"pn5xEMduV6ic","metadata":{"id":"pn5xEMduV6ic"},"outputs":[],"source":["# Fix DataTypes of the categorical columns with incorrect DataTypes\n","\n"]},{"cell_type":"code","execution_count":null,"id":"PeO79O6QsAIE","metadata":{"collapsed":true,"id":"PeO79O6QsAIE"},"outputs":[],"source":["# Check the final data types post conversion\n"]},{"cell_type":"markdown","id":"YFX9UUheLVQE","metadata":{"id":"YFX9UUheLVQE"},"source":["## **3. Exploratory Data Analysis**\n","\n","<font color = red>[27 marks]</font>"]},{"cell_type":"markdown","id":"UYbleboaLWWH","metadata":{"id":"UYbleboaLWWH"},"source":["### **3.1 Perform univariate analysis**\n","\n","<font color = red>[12 marks]</font>"]},{"cell_type":"markdown","id":"c_L22-lKWpzS","metadata":{"id":"c_L22-lKWpzS"},"source":["#### **3.1.1** Visualise the numerical features <font color = red>[5 marks]</font>\n","\n","Visualise the distribution of numerical features using appropriate plots to understand their characteristics."]},{"cell_type":"code","execution_count":null,"id":"k2kAF3VQL-4o","metadata":{"id":"k2kAF3VQL-4o"},"outputs":[],"source":["# Plot all the numerical columns to understand their distribution\n"]},{"cell_type":"markdown","id":"BF4n-SHrApij","metadata":{"id":"BF4n-SHrApij"},"source":["#### **3.1.2** Visualise the categorical features <font color = red>[5 marks]</font>\n","\n","Visualise the distribution of categorical features to get a clear view of the data distribution across various categories. This will help in identifying potential imbalances or dominant categories."]},{"cell_type":"code","execution_count":null,"id":"bfXnlpDhA7IS","metadata":{"id":"bfXnlpDhA7IS"},"outputs":[],"source":["# Select and plot categorical columns\n","\n"]},{"cell_type":"markdown","id":"492f151c","metadata":{"id":"492f151c"},"source":["#### **3.1.3** Check class distribution of the target feature <font color = red>[2 marks]</font>"]},{"cell_type":"code","execution_count":null,"id":"085f32a4","metadata":{"id":"085f32a4"},"outputs":[],"source":["# Class distribution of positive and negative classes\n"]},{"cell_type":"markdown","id":"TfCalcCgLooL","metadata":{"id":"TfCalcCgLooL"},"source":["### **3.2 Perform correlation analysis**\n","\n","<font color = red>[5 marks]</font>\n","\n","Investigate the relationships between numerical features to identify potential multicollinearity or dependencies. Visualise the correlation structure using an appropriate method to gain insights into feature relationships"]},{"cell_type":"markdown","id":"5dfc198d","metadata":{"id":"5dfc198d"},"source":["#### **3.2.1** Visualise the correlation among numerical features <font color=\"red\">[5 Marks]</font>\n"]},{"cell_type":"code","execution_count":null,"id":"LwN5fV8JMoZ4","metadata":{"collapsed":true,"id":"LwN5fV8JMoZ4"},"outputs":[],"source":["# Plot Heatmap of the correlation matrix\n"]},{"cell_type":"markdown","id":"j0mYPxt4Lh_6","metadata":{"id":"j0mYPxt4Lh_6"},"source":["### **3.3 Perform bivariate analysis**\n","\n","<font color = red>[10 marks]</font>"]},{"cell_type":"markdown","id":"49c25f39","metadata":{"id":"49c25f39"},"source":["#### **3.3.1** Analyse categorical features <font color=\"red\">[5 Marks]</font>\n","\n","For each categorical feature (excluding the target), calculate the proportion of `cardio = 1` in each category of the feature. Use this to identify which categorical features show clear differences in heart disease likelihood and which are less informative."]},{"cell_type":"code","execution_count":null,"id":"a9c4prDjYm08","metadata":{"collapsed":true,"id":"a9c4prDjYm08"},"outputs":[],"source":["# Write a function to analyse the target variable likelihood for categorical features\n","\n"]},{"cell_type":"markdown","id":"jIyPkJHYYlO5","metadata":{"id":"jIyPkJHYYlO5"},"source":["#### **3.3.2** Explore the relationships between numerical features and the target variable <font color = red>[5 marks]</font>\n","\n","Understand the impact of numeric features on the target outcome using appropriate visualisation techniques to identify trends and potential interactions"]},{"cell_type":"code","execution_count":null,"id":"OK4NV4DpMjUo","metadata":{"collapsed":true,"id":"OK4NV4DpMjUo"},"outputs":[],"source":["# Plot distribution for each numerical column with target variable\n"]},{"cell_type":"markdown","id":"6CnPejxMKeWU","metadata":{"id":"6CnPejxMKeWU"},"source":["## **4. Train-Test Split**\n","\n","<font color = red>[5 marks]</font>"]},{"cell_type":"markdown","id":"pt-doDvkKz6M","metadata":{"id":"pt-doDvkKz6M"},"source":["### **4.1 Data Splitting**\n","\n","<font color = red>[5 Marks]</font>"]},{"cell_type":"markdown","id":"c60a44f7","metadata":{"id":"c60a44f7"},"source":["#### **4.1.1** Define feature and target variables <font color = red>[2 Marks]</font>"]},{"cell_type":"code","execution_count":null,"id":"i9fkVgCJK3Dj","metadata":{"id":"i9fkVgCJK3Dj"},"outputs":[],"source":["# Put all the feature variables in X and target in y\n"]},{"cell_type":"markdown","id":"OFfHtJIBK3R8","metadata":{"id":"OFfHtJIBK3R8"},"source":["#### **4.1.2** Split the data into train and test sets <font color=\"red\">[3 Marks]</font>\n","\n","Split the data in 0.7:0.3 sets. and reset the index for the sets. Check the shape of the test and test sets.\n"]},{"cell_type":"code","execution_count":null,"id":"Q2hDut2zK651","metadata":{"id":"Q2hDut2zK651"},"outputs":[],"source":["#  Split the data into 70% train data and 30% test data\n"]},{"cell_type":"code","execution_count":null,"id":"n3QfNhdJwotC","metadata":{"id":"n3QfNhdJwotC"},"outputs":[],"source":["# Reset index for all train and test sets\n"]},{"cell_type":"markdown","id":"1dlFp6HfNYIz","metadata":{"id":"1dlFp6HfNYIz"},"source":["## **5. Feature Engineering**\n","\n","<font color = red>[18 marks]</font>"]},{"cell_type":"markdown","id":"bc1brat8KNIx","metadata":{"id":"bc1brat8KNIx"},"source":["### **5.1 Create a new feature**\n","\n","<font color = red>[6 marks]</font>"]},{"cell_type":"markdown","id":"xoua5dacriFg","metadata":{"id":"xoua5dacriFg"},"source":["#### **5.1.1** Create a new feature `BMI` (Body Mass Index) <font color=\"red\">[3 Marks]</font>\n","\n","BMI is a standard health metric calculated using a person's height and weight. BMI is known to be a useful predictor for cardiovascular risk."]},{"cell_type":"code","execution_count":null,"id":"MlW7YRbwKNaw","metadata":{"id":"MlW7YRbwKNaw"},"outputs":[],"source":["# Create a new feature 'BMI'\n"]},{"cell_type":"markdown","id":"MTJPpf7VsjPa","metadata":{"id":"MTJPpf7VsjPa"},"source":["**Note:** Feel free to engineer more features if you wish to."]},{"cell_type":"markdown","id":"LyViPF-dQRez","metadata":{"id":"LyViPF-dQRez"},"source":["#### **5.1.2** Perform correlation analysis  <font color=\"red\">[3 Marks]</font>\n","\n","After creating the new feature `BMI`, perform correlation analysis to check if it's correlated with any existing features. Perform suitable processing steps if high correlation is found."]},{"cell_type":"code","execution_count":null,"id":"ZTiZjUkBPl5Z","metadata":{"collapsed":true,"id":"ZTiZjUkBPl5Z"},"outputs":[],"source":["# Plot check correlation\n"]},{"cell_type":"code","execution_count":null,"id":"d49b9e2a","metadata":{"id":"d49b9e2a"},"outputs":[],"source":["# Did you find any highly correlated features? What steps should you take\n"]},{"cell_type":"markdown","id":"g4vihcwfQ9i4","metadata":{"id":"g4vihcwfQ9i4"},"source":["### **5.2 Combine Values in Categorical Columns**\n","\n","<font color=\"red\">[4 Marks]</font>"]},{"cell_type":"markdown","id":"5227f543","metadata":{"id":"5227f543"},"source":["#### **5.2.1** Combine Low-Frequency Categories <font color=\"red\">[4 Marks]</font>\n","\n","During the EDA process, categorical columns with multiple unique levels may be identified. To enhance model performance, it is recommended to refine these categorical features by grouping values that have low frequency or provide limited predictive information.\n","\n","Combine categories that occur infrequently or exhibit similar behavior to reduce sparsity and improve model generalisation."]},{"cell_type":"code","execution_count":null,"id":"tWfM9htIREdP","metadata":{"id":"tWfM9htIREdP"},"outputs":[],"source":[" # Combine categories that have low frequency or provide limited predictive information such as gluc and cholesterol\n","\n"]},{"cell_type":"markdown","id":"slfy24azGu4n","metadata":{"id":"slfy24azGu4n"},"source":["### **5.3 Dummy variable creation**\n","\n","<font color = red>[5 marks]</font>"]},{"cell_type":"markdown","id":"CKveMKeCTdH5","metadata":{"id":"CKveMKeCTdH5"},"source":["#### **5.3.1** Create dummy variables for categorical columns <font color=\"red\">[5 Mark]</font>"]},{"cell_type":"code","execution_count":null,"id":"A0-tavcUTdVL","metadata":{"id":"A0-tavcUTdVL"},"outputs":[],"source":["# Identify the columns for creating dummy variables\n"]},{"cell_type":"code","execution_count":null,"id":"GWtguJ5n9EH_","metadata":{"id":"GWtguJ5n9EH_"},"outputs":[],"source":["# Create dummy variables for independent columns on training data\n"]},{"cell_type":"code","execution_count":null,"id":"LFqETfMSLMpv","metadata":{"collapsed":true,"id":"LFqETfMSLMpv"},"outputs":[],"source":["# Create dummy variables for independent columns on test data\n"]},{"cell_type":"markdown","id":"yNHjF3RNPvdx","metadata":{"id":"yNHjF3RNPvdx"},"source":["### **5.4 Feature scaling**\n","\n","<font color = red>[3 marks]</font>"]},{"cell_type":"markdown","id":"6da08e5b","metadata":{"id":"6da08e5b"},"source":["#### **5.4.1** Scale numerical features <font color = red>[3 marks]</font>\n","\n","Choose a scaling method appropriate for the data and the chosen model. Apply the same scaling to both training and test data."]},{"cell_type":"code","execution_count":null,"id":"755ORaa3Pzdf","metadata":{"id":"755ORaa3Pzdf"},"outputs":[],"source":["# Scale the numeric features present in the training data\n","\n","\n","# Scale the numerical features present in the test data\n"]},{"cell_type":"code","execution_count":null,"id":"1_wA289rVV3B","metadata":{"id":"1_wA289rVV3B"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"psV0gGiz_Rgp","metadata":{"id":"psV0gGiz_Rgp"},"source":["## **6. Model Building**\n","\n","<font color = red>[38 marks]</font>"]},{"cell_type":"markdown","id":"a03d5090","metadata":{"id":"a03d5090"},"source":["In this task, you will build the two machine learning models: Support Vector Classifier (SVC) and a Decision Tree classifier. We will follow the same structured workflow for the models:\n","\n","* *Model Building and Initial Evaluation*: <br> Fit the model and evaluate its performance on the training data using the default cutoff\n","* *Find the Optimal Cutoff*: <br> Determine the best probability threshold using sensitivity-specificity and precision–recall trade-offs\n","* *Model Prediction & Evaluation using chosen cutoff*: <br> Generate predictions using the chosen cutoff and evaluate performance on the training data\n","* *Hyperparameter Tuning (Grid Search)*: <br> Optimise performance using grid search for hyperparameter tuning\n","* *Final Model Training & Evaluation using chosen cutoff*: <br> Train the final model using the best hyperparameters and evaluate performance on the training data"]},{"cell_type":"markdown","id":"169ecd43","metadata":{"id":"169ecd43"},"source":["### **6.1 SVM Classifier**\n","\n","<font color = red>[18 marks]</font>"]},{"cell_type":"markdown","id":"41312e4b","metadata":{"id":"41312e4b"},"source":["#### **6.1.1** Define a Linear SVM classifier and fit it on the train set <font color = red>[2 mark]</font>\n","\n","Go through the [SVC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and define a model with linear kernel that will also return the probabilities estimates of the predictions."]},{"cell_type":"code","execution_count":null,"id":"99e712c5","metadata":{"id":"99e712c5"},"outputs":[],"source":["# Define and fit linear SVM\n"]},{"cell_type":"markdown","id":"1f43d28f","metadata":{"id":"1f43d28f"},"source":["#### **6.1.2** Get the probability estimates on test set and predict class using a threshold <font color = red>[3 mark]</font>\n","\n","We defined the model to also return the probabilities after training. Use the `SVC.predict_proba()`[(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba) function to fetch the probabilities on test set. For each sample, it returns the probabilities of each class in a sorted order (according to `SVC.classes_`)\n","\n","After getting the probability values, assign class labels using the default threshold of 0.5 and check the distribution of assigned labels."]},{"cell_type":"code","execution_count":null,"id":"b63bd08d","metadata":{"id":"b63bd08d"},"outputs":[],"source":["# Use predict_proba() to get the probability of positive class for all data points\n"]},{"cell_type":"code","execution_count":null,"id":"c412d777","metadata":{"id":"c412d777"},"outputs":[],"source":["# Make class predictions based on default cutoff value of 0.5 on testing data\n"]},{"cell_type":"code","execution_count":null,"id":"b7930cf7","metadata":{"id":"b7930cf7"},"outputs":[],"source":["# check the counts of assigned labels\n"]},{"cell_type":"markdown","id":"88b676df","metadata":{"id":"88b676df"},"source":["#### **6.1.3** Predict the class labels using the `predict()` function <font color = red>[2 mark]</font>\n","\n","Now, directly use the `predict()` function to predict the class labels and check the distribution of assigned labels using this method."]},{"cell_type":"code","execution_count":null,"id":"4dd27d20","metadata":{"id":"4dd27d20"},"outputs":[],"source":["# Make class predictions using predict()\n"]},{"cell_type":"code","execution_count":null,"id":"60e5f6f2","metadata":{"id":"60e5f6f2"},"outputs":[],"source":["# check the counts of assigned labels\n"]},{"cell_type":"markdown","id":"66b661b1","metadata":{"id":"66b661b1"},"source":["Did you find any difference in the distribution of classes in the predictions using these two methods? Why do you think that is?\n","\n","Try going through the documentation of `predict_proba()` linked above."]},{"cell_type":"markdown","id":"7300834a","metadata":{"id":"7300834a"},"source":["#### **6.1.4** Calculate performance metrics for both the above methods <font color = red>[3 mark]</font>\n","\n","Calculate the performance metrics for both `predict_proba()` and `predict()` estimates. Compare the results and choose one to continue ahead."]},{"cell_type":"code","execution_count":null,"id":"ceaa4f00","metadata":{"id":"ceaa4f00"},"outputs":[],"source":["# check the performance for above two methods\n"]},{"cell_type":"markdown","id":"071262c8","metadata":{"id":"071262c8"},"source":["#### **6.1.5** Plot the ROC curve <font color=\"red\">[2 Marks]</font>\n","\n","Find the optimal cutoff to improve model performance by evaluating various cutoff values and their impact on relevant metrics. Plot ROC Curve to visualise the trade-off between true positive rate and false positive rate across different classification thresholds."]},{"cell_type":"code","execution_count":null,"id":"20a2bdcd","metadata":{"id":"20a2bdcd"},"outputs":[],"source":["# Plot the ROC curve\n","\n"]},{"cell_type":"markdown","id":"c76ef238","metadata":{"id":"c76ef238"},"source":["#### **6.1.6** Plot for accuracy, sensitivity, specificity at different probability cutoffs <font color=\"red\">[3 Marks]</font>"]},{"cell_type":"code","execution_count":null,"id":"e5cd91f8","metadata":{"id":"e5cd91f8"},"outputs":[],"source":["# Create a DataFrame to see the values of accuracy, sensitivity, and specificity at different values of probability cutoffs\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f092ba71","metadata":{"id":"f092ba71"},"outputs":[],"source":["# Plot accuracy, sensitivity, and specificity at different values of probability cutoffs stored in DF\n"]},{"cell_type":"markdown","id":"2e435732","metadata":{"id":"2e435732"},"source":["To minimise the risk of missing high cardiovascular risk individuals, we should prioritise our model's ability to correctly identify those with cardiovascular disease."]},{"cell_type":"markdown","id":"7f207f8d","metadata":{"id":"7f207f8d"},"source":["#### **6.1.7** Assign classes based on the optimal cutoff and evaluate <font color=\"red\">[2 Mark]</font>\n","\n","Finally, assign labels for both training and testing set, and calculate evaluation metrics for both to see if the model is overfitting."]},{"cell_type":"code","execution_count":null,"id":"87979b9e","metadata":{"id":"87979b9e"},"outputs":[],"source":["# Make final prediction based on the optimal cutoff\n"]},{"cell_type":"code","execution_count":null,"id":"2fb2e087","metadata":{"id":"2fb2e087"},"outputs":[],"source":["# Evaluate the model performance\n"]},{"cell_type":"code","execution_count":null,"id":"273b95c8","metadata":{"id":"273b95c8"},"outputs":[],"source":["# Check performance on training data\n","\n"]},{"cell_type":"markdown","id":"dee137c0","metadata":{"id":"dee137c0"},"source":["#### **6.1.8** Plot precision-recall curve <font color=\"red\">[1 Mark]</font>"]},{"cell_type":"code","execution_count":null,"id":"8bb8c706","metadata":{"id":"8bb8c706"},"outputs":[],"source":["# Compute precision–recall values and plot for various thresholds\n","\n"]},{"cell_type":"markdown","id":"4b0332b4","metadata":{"id":"4b0332b4"},"source":["Since we want to prioritise recall/sensitivity over precision to minimise the risk of missing high-risk individuals, we can choose an agreeable cutoff value."]},{"cell_type":"markdown","id":"18df32ac","metadata":{"id":"18df32ac"},"source":["### **6.2 Decision Tree Classifier**\n","\n","<font color = red>[12 marks]</font>"]},{"cell_type":"markdown","id":"8452ec1b","metadata":{"id":"8452ec1b"},"source":["#### **6.2.1** Define a Decision Tree classifier and fit it on the train set <font color = red>[1 mark]</font>"]},{"cell_type":"code","execution_count":null,"id":"7acb5a63","metadata":{"id":"7acb5a63"},"outputs":[],"source":["# Define and fit\n"]},{"cell_type":"markdown","id":"a778877e","metadata":{"id":"a778877e"},"source":["#### **6.2.2** Get feature importance scores <font color = red>[2 Marks]</font>"]},{"cell_type":"code","execution_count":null,"id":"29c519d3","metadata":{"id":"29c519d3"},"outputs":[],"source":["# Get feature importance scores from the trained model\n","\n"]},{"cell_type":"markdown","id":"1a87abfe","metadata":{"id":"1a87abfe"},"source":["#### **6.2.3** Predict the class probabilities on the test set <font color=\"red\">[1 Mark]</font>\n","\n","Use `predict_proba()` to get the probability estimates"]},{"cell_type":"code","execution_count":null,"id":"ce6cefd8","metadata":{"id":"ce6cefd8"},"outputs":[],"source":["# Predict the class probabilities\n","\n"]},{"cell_type":"markdown","id":"4351684c","metadata":{"id":"4351684c"},"source":["####  **6.2.4** Make prediction based on default cutoff value of 0.5 on testing data <font color = \"red\">[1 Mark]</font>"]},{"cell_type":"code","execution_count":null,"id":"845e5c04","metadata":{"id":"845e5c04"},"outputs":[],"source":["# Make prediction based on default cutoff value of 0.5\n","\n"]},{"cell_type":"markdown","id":"842aa294","metadata":{"id":"842aa294"},"source":["####  **6.2.5** Evaluate the performance of the model <font color = \"red\">[1 Mark]</font>"]},{"cell_type":"code","execution_count":null,"id":"dcfb4114","metadata":{"id":"dcfb4114"},"outputs":[],"source":["# Evaluate the performance of the model on training data\n","\n"]},{"cell_type":"markdown","id":"7026a3a6","metadata":{"id":"7026a3a6"},"source":["#### **6.2.6** Plot the ROC curve <font color=\"red\">[1 Marks]</font>\n","\n","Find the optimal cutoff to improve model performance by evaluating various cutoff values and their impact on relevant metrics. Plot ROC Curve to visualise the trade-off between true positive rate and false positive rate across different classification thresholds."]},{"cell_type":"code","execution_count":null,"id":"4d65d6e7","metadata":{"id":"4d65d6e7"},"outputs":[],"source":["# Plot the ROC curve\n"]},{"cell_type":"markdown","id":"62163f01","metadata":{"id":"62163f01"},"source":["**Sensitivity and Specificity tradeoff**\n","\n","Now check the sensitivity and specificity tradeoff to find the optimal cutoff point."]},{"cell_type":"markdown","id":"143f5468","metadata":{"id":"143f5468"},"source":["#### **6.2.7** Plot for accuracy, sensitivity, specificity at different probability cutoffs <font color=\"red\">[2 Marks]</font>"]},{"cell_type":"code","execution_count":null,"id":"e7c86f99","metadata":{"id":"e7c86f99"},"outputs":[],"source":["# Create a DataFrame to see the values of accuracy, sensitivity, and specificity at different values of probability cutoffs\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0a235b42","metadata":{"id":"0a235b42"},"outputs":[],"source":["# Plot accuracy, sensitivity, and specificity at different values of probability cutoffs\n","\n"]},{"cell_type":"markdown","id":"0d8a59a0","metadata":{"id":"0d8a59a0"},"source":["#### **6.2.8** Assign classes based on the optimal cutoff and evaluate <font color=\"red\">[2 Mark]</font>\n","\n","Finally, assign labels for both training and testing set, and calculate evaluation metrics for both to see if the model is overfitting."]},{"cell_type":"code","execution_count":null,"id":"bdd2707b","metadata":{"id":"bdd2707b"},"outputs":[],"source":["# Make final prediction based on the optimal cutoff\n"]},{"cell_type":"code","execution_count":null,"id":"ff9522b0","metadata":{"id":"ff9522b0"},"outputs":[],"source":["# Evaluate the model performance for test and train\n"]},{"cell_type":"markdown","id":"ecb827a1","metadata":{"id":"ecb827a1"},"source":["**Precision and Recall tradeoff**\n","\n","Check optimal cutoff value by plotting precision-recall curve, and adjust the cutoff based on precision and recall tradeoff if required."]},{"cell_type":"markdown","id":"2f789b90","metadata":{"id":"2f789b90"},"source":["#### **6.2.9** Plot precision-recall curve <font color=\"red\">[1 Mark]</font>"]},{"cell_type":"code","execution_count":null,"id":"c7f4eca5","metadata":{"id":"c7f4eca5"},"outputs":[],"source":["# Compute and plot precision–recall values\n"]},{"cell_type":"markdown","id":"a0dcaccf","metadata":{"id":"a0dcaccf"},"source":["#### **6.2.10** Build another model of your choice.\n","\n","Optionally, build a third classification model of your choice and compare its performance on training and testing sets with the first two models."]},{"cell_type":"code","execution_count":null,"id":"4d1e867e","metadata":{"id":"4d1e867e"},"outputs":[],"source":["# Third model of your choice\n"]},{"cell_type":"code","execution_count":null,"id":"f2ccd316","metadata":{"id":"f2ccd316"},"outputs":[],"source":["# Evaluate and compare\n"]},{"cell_type":"markdown","id":"82a7f144","metadata":{"id":"82a7f144"},"source":["### **6.3 Hyperparameter Tuning**\n","\n","<font color = red>[8 Marks]</font>\n","\n","Enhance the performance of the decision tree model by systematically exploring and selecting optimal hyperparameter values using Grid Search."]},{"cell_type":"markdown","id":"15477b65","metadata":{"id":"15477b65"},"source":["#### **6.3.1** Use grid search to find the best hyperparameter values <font color = red>[4 Marks]</font>\n","\n","Perform hyperparameter tuning to see if the performance of the decision tree model can be improved. Tune for **at least 4 decision tree hyperparameters**."]},{"cell_type":"code","execution_count":null,"id":"8a0b23c8","metadata":{"id":"8a0b23c8"},"outputs":[],"source":["# Use grid search to find best hyperparameters for decision tree model\n","\n","# Define the parameter grid for the decision tree\n","\n","\n","\n","# Print the best hyperparameters\n"]},{"cell_type":"markdown","id":"b7d611d1","metadata":{"id":"b7d611d1"},"source":["#### **6.3.2** Build a decision tree model based on hyperparameter tuning results <font color = red>[2 Marks]</font>\n"]},{"cell_type":"code","execution_count":null,"id":"a54d2e7a","metadata":{"id":"a54d2e7a"},"outputs":[],"source":["# Use the best DT from grid search\n"]},{"cell_type":"markdown","id":"7b39f179","metadata":{"id":"7b39f179"},"source":["#### **6.3.3** Using the tuned model, make predictions and evaluate <font color=\"red\">[2 Mark]</font>\n","\n","Use the tuned model to directly predict the labels and evaluate the performance on both training and testing sets to check overfitting / underfitting."]},{"cell_type":"code","execution_count":null,"id":"4408250d","metadata":{"id":"4408250d"},"outputs":[],"source":["# Evaluate the model performance on training set\n"]},{"cell_type":"code","execution_count":null,"id":"8af32560","metadata":{"id":"8af32560"},"outputs":[],"source":["# Evaluate the model performance on test set\n"]},{"cell_type":"markdown","id":"b80b4572","metadata":{"id":"b80b4572"},"source":["#### **6.3.4** Optionally, use grid search to find the best hyperparameter values for SVM\n","\n","Try to fine-tune SVM hyperparameters like kernels, `C` and `gamma`."]},{"cell_type":"code","execution_count":null,"id":"b6f414d8","metadata":{"id":"b6f414d8"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ee39ecb4","metadata":{"id":"ee39ecb4"},"source":["You can also check the performance of SVM with `RBF` kernel"]},{"cell_type":"code","execution_count":null,"id":"99850b8f","metadata":{"id":"99850b8f"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"b798e14f","metadata":{"id":"b798e14f"},"source":["Tune your third candidate model, if taken"]},{"cell_type":"code","execution_count":null,"id":"1dec99a2","metadata":{"id":"1dec99a2"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"91ZO8KfWQXW8","metadata":{"id":"91ZO8KfWQXW8"},"source":["## **7. Final Model Evaluation and Selection**\n","\n","<font color = red>[2 Marks]</font>\n","\n","Use you final models to make predictions on the test data. Evaluate the models, create model cards, and finally write your conclusive findings, results, and insights from the steps performed.\n","\n","Include these in your report as well."]},{"cell_type":"markdown","id":"kQyxxDLrx2Pm","metadata":{"id":"kQyxxDLrx2Pm"},"source":["### **7.1 Evaluate the final models**\n","\n","<font color = red>[2 Marks]</font>\n","\n","Make predictions using the tuned models and selected features to check the training and testing performances and create model cards for both."]},{"cell_type":"markdown","id":"Ly_N6IskQXW9","metadata":{"id":"Ly_N6IskQXW9"},"source":["#### **7.1.1** Make final predictions and evaluate <font color=\"red\">[2 Marks]</font>\n","\n","Evaluate the performance of your final candidates"]},{"cell_type":"code","execution_count":null,"id":"c8e8188f","metadata":{"id":"c8e8188f"},"outputs":[],"source":["# Make predictions on test and train sets using all candidate models\n","# use the chosen optimal cutoff\n"]},{"cell_type":"markdown","id":"2807bfde","metadata":{"id":"2807bfde"},"source":["### **7.2 Conclusion**"]},{"cell_type":"markdown","id":"HWffnJ08u9Pc","metadata":{"id":"HWffnJ08u9Pc"},"source":["#### **7.2.1** Model Cards\n","\n","Create model cards for all your candidate models. Include this in your report."]},{"cell_type":"markdown","id":"b20e58c6","metadata":{"id":"b20e58c6"},"source":["Use the following as a general-purpose template for supervised ML model documentation:\n","\n","\n","**Model Card: [Model name]**\n","\n","**Model overview:**\n","Brief description of the model, its purpose, and context.\n","\n","**Intended use:**\n","\n","* Primary task and problem type\n","* Intended users\n","* Suitable deployment or research settings\n","\n","**Data and features:**\n","\n","* Summary of raw features\n","* Engineered or transformed features\n","* Preprocessing choices, including dropped or merged variables and rationale\n","\n","**Model configuration:**\n","\n","* Algorithm type\n","* Key hyperparameters\n","* Training details (scaling, class weights, thresholds, calibration)\n","\n","**Performance:**\n","\n","* Train metrics (optional)\n","* Validation/test metrics using consistent thresholds\n","* Notes on strengths, weaknesses, and observed behaviour\n","\n","**Limitations and considerations:**\n","\n","* Interpretability constraints\n","* Error risks (false positives/negatives)\n","* Fairness considerations\n","* Operational or domain-specific caveats\n","---"]},{"cell_type":"markdown","id":"d870ed54","metadata":{"id":"d870ed54"},"source":["#### **7.2.2** Conclusions and Outcomes\n","\n","Try to answer the following questions in your answer. Include this in the report.\n","\n","What insights did you find in EDA and what feature engineering steps were performed? Describe your choice of models and the performance of baseline models. Did you find overfitting? How was it handled and what was the result of tuning? Was the data sufficent? Is a linear model sufficient? What model did you choose? Explain the final outcomes."]},{"cell_type":"markdown","id":"5da8e703","metadata":{"id":"5da8e703"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":5}